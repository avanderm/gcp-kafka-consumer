/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package kafka.consumer

import com.google.protobuf.GeneratedMessageV3
import com.google.protobuf.Descriptors
import com.google.protobuf.Parser
import org.apache.avro.Schema
import org.apache.avro.SchemaBuilder
import org.apache.avro.generic.GenericRecord
import org.apache.avro.generic.GenericRecordBuilder
import org.apache.beam.sdk.Pipeline
import org.apache.beam.sdk.coders.AvroCoder
import org.apache.beam.sdk.coders.NullableCoder
import org.apache.beam.sdk.options.Default
import org.apache.beam.sdk.options.Description
import org.apache.beam.sdk.options.PipelineOptions
import org.apache.beam.sdk.options.PipelineOptionsFactory
import org.apache.beam.sdk.options.Validation.Required
import org.apache.beam.sdk.extensions.protobuf.ProtoCoder
import org.apache.beam.sdk.io.Compression
import org.apache.beam.sdk.io.FileIO
import org.apache.beam.sdk.io.kafka.KafkaIO
import org.apache.beam.sdk.io.kafka.KafkaRecord
import org.apache.beam.sdk.io.kafka.KafkaRecordCoder
import org.apache.beam.sdk.io.parquet.ParquetIO
import org.apache.beam.sdk.transforms.DoFn
import org.apache.beam.sdk.transforms.ParDo
import org.apache.beam.sdk.transforms.windowing.FixedWindows
import org.apache.beam.sdk.transforms.windowing.Window
import org.apache.beam.sdk.transforms.windowing.BoundedWindow
import org.apache.beam.sdk.transforms.windowing.PaneInfo
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.kafka.common.serialization.Deserializer
import org.joda.time.Duration
import org.joda.time.format.DateTimeFormat
import org.joda.time.format.DateTimeFormatter
import org.joda.time.DateTimeZone
import org.joda.time.LocalDateTime

class Consumer {
    companion object {
        const val STORAGE_PREFIX = "kafka_sink"
    }

    public class ProtoDeserializer : Deserializer<GeneratedMessageV3> {
        lateinit var parser: Parser<out GeneratedMessageV3>

        override fun configure(configs: Map<String, Any?>, isKey: Boolean): Unit {
            println(configs)
            if (isKey) {
                val loadedClass = Thread.currentThread().contextClassLoader.loadClass(configs.get("keyClass") as String) as Class<out GeneratedMessageV3>
                this.parser = loadedClass.getDeclaredMethod("parser").invoke(null) as Parser<out GeneratedMessageV3>
            } else {
                val loadedClass = Thread.currentThread().contextClassLoader.loadClass(configs.get("valueClass") as String) as Class<out GeneratedMessageV3>
                this.parser = loadedClass.getDeclaredMethod("parser").invoke(null) as Parser<out GeneratedMessageV3>
            }
        }

        override fun deserialize(topic: String, data: ByteArray?): GeneratedMessageV3? {
            if (data != null)
                return this.parser.parseFrom(data) as GeneratedMessageV3

            return null
        }
    }

    interface ConsumerOptions : PipelineOptions {
        @get:Description("Kafka bootstrap servers")
        @get:Required
        var bootstrapServers: String

        @get:Description("Cloud Storage bucket")
        @get:Required
        var bucket: String

        @get:Description("Cloud Storage prefix")
        @get:Default.String(STORAGE_PREFIX)
        var storagePrefix: String

        @get:Description("Kafka topic to consume")
        @get:Required
        var kafkaTopic: String

        @get:Description("Kafka consumer group")
        @get:Required
        var consumerGroup: String

        @get:Description("Protobuf value class")
        @get:Required
        var protobufValueClass: String

        @get:Description("Protobuf key class")
        @get:Required
        var protobufKeyClass: String
    }

    fun convertSchema(recordName: String, descriptor: Descriptors.Descriptor, addKafkaMetadata: Boolean = false) : Schema {
        var fieldAssembler = SchemaBuilder
            .record(recordName).namespace("hp")
            .fields()

        descriptor.fields.forEach {
            fieldAssembler = when(it.type) {
                Descriptors.FieldDescriptor.Type.BOOL ->
                    fieldAssembler.optionalBoolean(it.name)
                Descriptors.FieldDescriptor.Type.DOUBLE ->
                    fieldAssembler.optionalDouble(it.name)
                Descriptors.FieldDescriptor.Type.MESSAGE ->
                    fieldAssembler.name(it.name).type(convertSchema(it.name, it.getMessageType(), false)).noDefault()
                Descriptors.FieldDescriptor.Type.STRING ->
                    fieldAssembler.optionalString(it.name)
                Descriptors.FieldDescriptor.Type.UINT32 ->
                    fieldAssembler.optionalInt(it.name)
                else ->
                    throw Exception("Cannot map protobuf type $it.type to a AVRO field type")
            }
        }

        if (addKafkaMetadata) {
            val metadataSchema = SchemaBuilder
                .record("kafka").namespace("hp")
                .fields()
                    .optionalInt("partition")
                    .optionalLong("offset")
                    .optionalLong("timestamp")
                    .optionalString("operation")
                .endRecord()

            fieldAssembler = fieldAssembler.name("kafka").type(metadataSchema).noDefault()
        }

        return fieldAssembler.endRecord()
    }

    fun run(args: Array<String>) {
        val options = PipelineOptionsFactory
            .fromArgs(*args)
            .withValidation()
            .`as`(ConsumerOptions::class.java)
        val pipeline = Pipeline.create(options)

        val loadedValueClass = Thread.currentThread().contextClassLoader.loadClass(options.protobufValueClass) as Class<out GeneratedMessageV3>

        val descriptor = loadedValueClass.getDeclaredMethod("getDescriptor").invoke(null) as Descriptors.Descriptor

        val valueSchema = convertSchema("message", descriptor, true)

        val input = pipeline.apply(
            KafkaIO.read<GeneratedMessageV3, GeneratedMessageV3>()
                .withBootstrapServers(options.bootstrapServers)
                .withTopic(options.kafkaTopic)
                .withKeyDeserializer(ProtoDeserializer::class.java)
                .withValueDeserializer(ProtoDeserializer::class.java)
                .withConsumerConfigUpdates(
                    mapOf(
                        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG to "earliest",
                        ConsumerConfig.RECEIVE_BUFFER_CONFIG to (512 * 1024),
                        ConsumerConfig.GROUP_ID_CONFIG to options.consumerGroup,
                        ConsumerConfig.MAX_POLL_RECORDS_CONFIG to 20000
                    )
                )
                .updateConsumerProperties(
                    mapOf(
                        "valueClass" to options.protobufValueClass,
                        "keyClass" to options.protobufKeyClass
                    )
                )
                .withReadCommitted()
                .commitOffsetsInFinalize())

        // val keyCoder = ProtoCoder.of(loadedKeyClass)
        // // convention: deletes will have value set to null
        // val valueCoder = NullableCoder.of(ProtoCoder.of(loadedValueClass))
        // val kafkaCoder = KafkaRecordCoder(keyCoder, valueCoder)
        // input.setCoder(kafkaCoder)

        val enrichedValues = input.apply(
            ParDo.of(EnrichFn(valueSchema))
        ).setCoder(AvroCoder.of(valueSchema))

        // Batch output in 10 minute windows
        val windowedValues = enrichedValues.apply(
            Window.into<GenericRecord>(FixedWindows.of(Duration.standardMinutes(10)))
        )

        val partitionedFilenameCreator = customPartitionedFilenameCreator(options.consumerGroup, options.storagePrefix, ".parquet");

        // Write to file
        windowedValues.apply(FileIO
            .write<GenericRecord>()
            .withNumShards(1)
            .via(ParquetIO.sink(valueSchema))
            .to(String.format("gs://%s/", options.bucket))
            .withSuffix(".parquet")
        )

        pipeline.run()
    }

    public class EnrichFn() : DoFn<KafkaRecord<GeneratedMessageV3, GeneratedMessageV3>, GenericRecord>() {
        lateinit var schema: Schema
        lateinit var schemaJson: String

        constructor(schema: Schema) : this() {
            this.schemaJson = schema.toString()
        }

        @Setup
        fun setup() {
            this.schema = Schema.Parser().parse(this.schemaJson)
        }

        fun transformRecord(record: GeneratedMessageV3?, schema: Schema, kafkaMetadata: GenericRecord?) : GenericRecord? {
            if (record == null)
                return null

            val recordBuilder = GenericRecordBuilder(schema)

            record.getDescriptorForType().getFields().forEach {
                if (record.hasField(it)) {
                    val value = record.getField(it)
                    if (value is GeneratedMessageV3) {
                        recordBuilder.set(it.name, transformRecord(value, schema.getField(it.name).schema(), null))
                    } else {
                        recordBuilder.set(it.name, value)
                    }
                }
            }

            if (kafkaMetadata != null) {
                recordBuilder.set("kafka", kafkaMetadata)
            }

            return recordBuilder.build()
        }

        @ProcessElement
        fun processElement(c: ProcessContext) {
            try{
                val kafkaRecord = c.element()
                val key = kafkaRecord.getKV().getKey()
                val value = kafkaRecord.getKV().getValue()

                val operation = if (value != null) "UPSERT" else "DELETE"
                val metadataSchema = this.schema.getField("kafka").schema()
                val metadataRecord = GenericRecordBuilder(metadataSchema)
                    .set("partition", kafkaRecord.getPartition())
                    .set("offset", kafkaRecord.getOffset())
                    .set("timestamp", kafkaRecord.getTimestamp())
                    .set("operation", operation)
                    .build()

                val genericRecord = when(operation) {
                    "UPSERT" -> transformRecord(value, this.schema, metadataRecord)
                    "DELETE" -> {
                        val transformedKey = transformRecord(key, this.schema.getField("key").schema(), null)
                        GenericRecordBuilder(this.schema)
                            .set("key", transformedKey)
                            .set("kafka", metadataRecord)
                            .build()
                    }
                    else -> throw Exception("What the fuck")
                }

                c.output(genericRecord)
            } catch(e: Exception) {
                e.printStackTrace()
                println("Nope")
            }
        }
    }

}

fun main(args: Array<String>) {
    Consumer().run(args)
}
